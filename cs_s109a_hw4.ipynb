{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2021-s109a/blob/master/lectures/crest.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Homework 4:  Logistic Regression and PCA\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Summer 2021**<br/>\n",
    "**Instructors**: Kevin Rader\n",
    "\n",
    "\n",
    "<hr style='height:2px'>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #fce8e8;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "- As much as possible, try and stick to the hints and functions we import at the top of the homework, as those are the ideas and tools the class supports and is aiming to teach. And if a problem specifies a particular library you're required to use that library, and possibly others from the import list.\n",
    "- Please use .head() when viewing data. Do not submit a notebook that is excessively long because output was not suppressed or otherwise limited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import zipfile\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# if you want to do a 2-sample t-test:\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='theme'> Cancer Classification from Gene Expressions </div>\n",
    "\n",
    "In this problem, we will build a classification model to distinguish between two related classes of cancer, acute lymphoblastic leukemia (ALL) and acute myeloid leukemia (AML), using gene expression measurements. The dataset is provided in the file `data/genomic_data.csv`. Each row in this file corresponds to a tumor tissue sample from a patient with one of the two forms of Leukemia. The first column contains the cancer type, with **0 indicating the ALL** class and **1 indicating the AML** class. Columns 2-7130 contain expression levels of 7129 genes recorded from each tissue sample. \n",
    "\n",
    "In the following questions, we will use linear and logistic regression to build classification models for this data set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 1 [15 pts]: Data Exploration </b></div>\n",
    "\n",
    "The first step is to split the observations into an approximate 75-25 train-test split.  Below is some code to do this for you (we want to make sure everyone has the same splits). It also prints the dataset's shape before splitting and after splitting. `Cancer_type` is our target column.\n",
    "\n",
    "\n",
    "**1.1** Take a peek at your training set: you should notice the severe differences in the measurements from one gene to the next (some are negative, some hover around zero, and some are well into the thousands). To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1. **NOTE: for the entirety of this homework assignment, you will use these normalized values, not the original, raw values**. Normalizing genomic data is a fairly standard first step.\n",
    "\n",
    "\n",
    "**1.2** The training set contains more predictors than observations. What problem(s) can this lead to in fitting a classification model to such a dataset? Explain in 3 or fewer sentences.\n",
    "\n",
    "\n",
    "**1.3** Determine which single gene individually discriminates between the two cancer classes the best (consider every gene in the dataset) and call it `best_predictor`.\n",
    "\n",
    "Plot two histograms of your `best_predictor` -- one using the training set and another using the testing set. The histogram should clearly distinguish two different `Cancer_type` classes.\n",
    "\n",
    "**Hint:** You may use any reasonable approach to determine the `best_predictor`, but please use something very simple (whether taught in this class or elsewhere).\n",
    "\n",
    "\n",
    "**1.4** Using `best_predictor`, create a classification model by simply eye-balling a value for this gene that would discriminate the two classes the best (do not use an algorithm to determine for you the optimal coefficient or threshold; we are asking you to provide a rough estimate / model by manual inspection). Justify your choice in 1-2 sentences. Report the accuracy of your hand-chosen model on the test set.\n",
    "\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first step is to split the observations into an approximate 75-25 train-test split. Below is some code to do this for you (we want to make sure everyone has the same splits). Print dataset shape before splitting and after splitting. `Cancer_type` is our target column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "df = pd.read_csv('data/genomic_data.csv', index_col=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns != 'Cancer_type'], \n",
    "                                                         df.Cancer_type, test_size=0.25, \n",
    "                                                         random_state = 109, \n",
    "                                                         stratify = df.Cancer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of full set: (752, 7130)\n",
      "shape of x_train: (564, 7129) \n",
      "shape of x_test: (188, 7129) \n",
      "shape of y_train: (564,) \n",
      "shape of y_test: (188,)\n",
      "value count of dataset: \n",
      " 0.0    0.511968\n",
      "1.0    0.488032\n",
      "Name: Cancer_type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('shape of full set:', df.shape)\n",
    "print('shape of x_train:', X_train.shape,'\\nshape of x_test:',X_test.shape,'\\nshape of y_train:', y_train.shape,'\\nshape of y_test:', y_test.shape)\n",
    "print('value count of dataset: \\n', df.Cancer_type.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Take a peek at your training set: you should notice the severe differences in the measurements from one gene to the next (some are negative, some hover around zero, and some are well into the thousands). To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1. **NOTE: for the entirety of this homework assignment, you will use these normalized values, not the original, raw values.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cancer_type</th>\n",
       "      <th>AFFX-BioB-5_at</th>\n",
       "      <th>AFFX-BioB-M_at</th>\n",
       "      <th>AFFX-BioB-3_at</th>\n",
       "      <th>AFFX-BioC-5_at</th>\n",
       "      <th>AFFX-BioC-3_at</th>\n",
       "      <th>AFFX-BioDn-5_at</th>\n",
       "      <th>AFFX-BioDn-3_at</th>\n",
       "      <th>AFFX-CreX-5_at</th>\n",
       "      <th>AFFX-CreX-3_at</th>\n",
       "      <th>...</th>\n",
       "      <th>U48730_at</th>\n",
       "      <th>U58516_at</th>\n",
       "      <th>U73738_at</th>\n",
       "      <th>X06956_at</th>\n",
       "      <th>X16699_at</th>\n",
       "      <th>X83863_at</th>\n",
       "      <th>Z17240_at</th>\n",
       "      <th>L49218_f_at</th>\n",
       "      <th>M71243_f_at</th>\n",
       "      <th>Z78285_f_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482348</td>\n",
       "      <td>0.636828</td>\n",
       "      <td>0.421504</td>\n",
       "      <td>0.354724</td>\n",
       "      <td>0.541964</td>\n",
       "      <td>0.313961</td>\n",
       "      <td>0.656215</td>\n",
       "      <td>0.557102</td>\n",
       "      <td>0.751598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577873</td>\n",
       "      <td>0.430681</td>\n",
       "      <td>0.346731</td>\n",
       "      <td>0.386057</td>\n",
       "      <td>0.611047</td>\n",
       "      <td>0.499519</td>\n",
       "      <td>0.370208</td>\n",
       "      <td>0.549853</td>\n",
       "      <td>0.470914</td>\n",
       "      <td>0.434172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538320</td>\n",
       "      <td>0.663173</td>\n",
       "      <td>0.653266</td>\n",
       "      <td>0.302249</td>\n",
       "      <td>0.463755</td>\n",
       "      <td>0.298072</td>\n",
       "      <td>0.644340</td>\n",
       "      <td>0.479530</td>\n",
       "      <td>0.602312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624184</td>\n",
       "      <td>0.522397</td>\n",
       "      <td>0.715986</td>\n",
       "      <td>0.349330</td>\n",
       "      <td>0.498395</td>\n",
       "      <td>0.471359</td>\n",
       "      <td>0.310147</td>\n",
       "      <td>0.387588</td>\n",
       "      <td>0.460523</td>\n",
       "      <td>0.317450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.558867</td>\n",
       "      <td>0.655742</td>\n",
       "      <td>0.408589</td>\n",
       "      <td>0.409961</td>\n",
       "      <td>0.582961</td>\n",
       "      <td>0.475211</td>\n",
       "      <td>0.599733</td>\n",
       "      <td>0.611503</td>\n",
       "      <td>0.617755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553454</td>\n",
       "      <td>0.469745</td>\n",
       "      <td>0.542662</td>\n",
       "      <td>0.405702</td>\n",
       "      <td>0.628022</td>\n",
       "      <td>0.407405</td>\n",
       "      <td>0.364542</td>\n",
       "      <td>0.508344</td>\n",
       "      <td>0.449332</td>\n",
       "      <td>0.460110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582957</td>\n",
       "      <td>0.642908</td>\n",
       "      <td>0.633893</td>\n",
       "      <td>0.331939</td>\n",
       "      <td>0.476370</td>\n",
       "      <td>0.318081</td>\n",
       "      <td>0.636519</td>\n",
       "      <td>0.554080</td>\n",
       "      <td>0.658937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447358</td>\n",
       "      <td>0.517868</td>\n",
       "      <td>0.289674</td>\n",
       "      <td>0.345401</td>\n",
       "      <td>0.640367</td>\n",
       "      <td>0.558384</td>\n",
       "      <td>0.428003</td>\n",
       "      <td>0.500796</td>\n",
       "      <td>0.499371</td>\n",
       "      <td>0.423364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341354</td>\n",
       "      <td>0.564548</td>\n",
       "      <td>0.468144</td>\n",
       "      <td>0.292583</td>\n",
       "      <td>0.386808</td>\n",
       "      <td>0.177429</td>\n",
       "      <td>0.518919</td>\n",
       "      <td>0.267971</td>\n",
       "      <td>0.617755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665444</td>\n",
       "      <td>0.464084</td>\n",
       "      <td>0.388716</td>\n",
       "      <td>0.371196</td>\n",
       "      <td>0.703637</td>\n",
       "      <td>0.670006</td>\n",
       "      <td>0.414405</td>\n",
       "      <td>0.593250</td>\n",
       "      <td>0.565236</td>\n",
       "      <td>0.557378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.696528</td>\n",
       "      <td>0.584498</td>\n",
       "      <td>0.379790</td>\n",
       "      <td>0.437056</td>\n",
       "      <td>0.422404</td>\n",
       "      <td>0.294715</td>\n",
       "      <td>0.631741</td>\n",
       "      <td>0.330545</td>\n",
       "      <td>0.747420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560462</td>\n",
       "      <td>0.651464</td>\n",
       "      <td>0.435819</td>\n",
       "      <td>0.311584</td>\n",
       "      <td>0.508865</td>\n",
       "      <td>0.516085</td>\n",
       "      <td>0.323915</td>\n",
       "      <td>0.654308</td>\n",
       "      <td>0.857588</td>\n",
       "      <td>0.324064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.640335</td>\n",
       "      <td>0.709007</td>\n",
       "      <td>0.145601</td>\n",
       "      <td>0.640275</td>\n",
       "      <td>0.543410</td>\n",
       "      <td>0.819101</td>\n",
       "      <td>0.498926</td>\n",
       "      <td>0.753030</td>\n",
       "      <td>0.840198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793460</td>\n",
       "      <td>0.295969</td>\n",
       "      <td>0.402568</td>\n",
       "      <td>0.676392</td>\n",
       "      <td>0.560715</td>\n",
       "      <td>0.575147</td>\n",
       "      <td>0.336958</td>\n",
       "      <td>0.699631</td>\n",
       "      <td>0.752875</td>\n",
       "      <td>0.557553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.519750</td>\n",
       "      <td>0.459566</td>\n",
       "      <td>0.198037</td>\n",
       "      <td>0.314083</td>\n",
       "      <td>0.582719</td>\n",
       "      <td>0.369624</td>\n",
       "      <td>0.658192</td>\n",
       "      <td>0.444315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506640</td>\n",
       "      <td>0.212257</td>\n",
       "      <td>0.585057</td>\n",
       "      <td>0.331757</td>\n",
       "      <td>0.773867</td>\n",
       "      <td>0.800121</td>\n",
       "      <td>0.176060</td>\n",
       "      <td>0.352877</td>\n",
       "      <td>0.379286</td>\n",
       "      <td>0.726550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.622598</td>\n",
       "      <td>0.732820</td>\n",
       "      <td>0.621132</td>\n",
       "      <td>0.303219</td>\n",
       "      <td>0.065618</td>\n",
       "      <td>0.375281</td>\n",
       "      <td>0.557282</td>\n",
       "      <td>0.373386</td>\n",
       "      <td>0.490326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559916</td>\n",
       "      <td>0.536638</td>\n",
       "      <td>0.238878</td>\n",
       "      <td>0.297934</td>\n",
       "      <td>0.806955</td>\n",
       "      <td>0.564062</td>\n",
       "      <td>0.510765</td>\n",
       "      <td>0.436667</td>\n",
       "      <td>0.348156</td>\n",
       "      <td>0.405826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.566346</td>\n",
       "      <td>0.749340</td>\n",
       "      <td>0.520202</td>\n",
       "      <td>0.586949</td>\n",
       "      <td>0.549270</td>\n",
       "      <td>0.179827</td>\n",
       "      <td>0.547697</td>\n",
       "      <td>0.673164</td>\n",
       "      <td>0.734504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614662</td>\n",
       "      <td>0.400230</td>\n",
       "      <td>0.437619</td>\n",
       "      <td>0.376556</td>\n",
       "      <td>0.671358</td>\n",
       "      <td>0.803143</td>\n",
       "      <td>0.303995</td>\n",
       "      <td>0.243517</td>\n",
       "      <td>0.534543</td>\n",
       "      <td>0.648342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752 rows Ã— 7130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cancer_type  AFFX-BioB-5_at  AFFX-BioB-M_at  AFFX-BioB-3_at  \\\n",
       "0            0.0        0.482348        0.636828        0.421504   \n",
       "1            0.0        0.538320        0.663173        0.653266   \n",
       "2            0.0        0.558867        0.655742        0.408589   \n",
       "3            0.0        0.582957        0.642908        0.633893   \n",
       "4            0.0        0.341354        0.564548        0.468144   \n",
       "..           ...             ...             ...             ...   \n",
       "747          0.0        0.696528        0.584498        0.379790   \n",
       "748          1.0        0.640335        0.709007        0.145601   \n",
       "749          1.0        0.519750        0.459566        0.198037   \n",
       "750          0.0        0.622598        0.732820        0.621132   \n",
       "751          1.0        0.566346        0.749340        0.520202   \n",
       "\n",
       "     AFFX-BioC-5_at  AFFX-BioC-3_at  AFFX-BioDn-5_at  AFFX-BioDn-3_at  \\\n",
       "0          0.354724        0.541964         0.313961         0.656215   \n",
       "1          0.302249        0.463755         0.298072         0.644340   \n",
       "2          0.409961        0.582961         0.475211         0.599733   \n",
       "3          0.331939        0.476370         0.318081         0.636519   \n",
       "4          0.292583        0.386808         0.177429         0.518919   \n",
       "..              ...             ...              ...              ...   \n",
       "747        0.437056        0.422404         0.294715         0.631741   \n",
       "748        0.640275        0.543410         0.819101         0.498926   \n",
       "749        0.314083        0.582719         0.369624         0.658192   \n",
       "750        0.303219        0.065618         0.375281         0.557282   \n",
       "751        0.586949        0.549270         0.179827         0.547697   \n",
       "\n",
       "     AFFX-CreX-5_at  AFFX-CreX-3_at  ...  U48730_at  U58516_at  U73738_at  \\\n",
       "0          0.557102        0.751598  ...   0.577873   0.430681   0.346731   \n",
       "1          0.479530        0.602312  ...   0.624184   0.522397   0.715986   \n",
       "2          0.611503        0.617755  ...   0.553454   0.469745   0.542662   \n",
       "3          0.554080        0.658937  ...   0.447358   0.517868   0.289674   \n",
       "4          0.267971        0.617755  ...   0.665444   0.464084   0.388716   \n",
       "..              ...             ...  ...        ...        ...        ...   \n",
       "747        0.330545        0.747420  ...   0.560462   0.651464   0.435819   \n",
       "748        0.753030        0.840198  ...   0.793460   0.295969   0.402568   \n",
       "749        0.444315        1.000000  ...   0.506640   0.212257   0.585057   \n",
       "750        0.373386        0.490326  ...   0.559916   0.536638   0.238878   \n",
       "751        0.673164        0.734504  ...   0.614662   0.400230   0.437619   \n",
       "\n",
       "     X06956_at  X16699_at  X83863_at  Z17240_at  L49218_f_at  M71243_f_at  \\\n",
       "0     0.386057   0.611047   0.499519   0.370208     0.549853     0.470914   \n",
       "1     0.349330   0.498395   0.471359   0.310147     0.387588     0.460523   \n",
       "2     0.405702   0.628022   0.407405   0.364542     0.508344     0.449332   \n",
       "3     0.345401   0.640367   0.558384   0.428003     0.500796     0.499371   \n",
       "4     0.371196   0.703637   0.670006   0.414405     0.593250     0.565236   \n",
       "..         ...        ...        ...        ...          ...          ...   \n",
       "747   0.311584   0.508865   0.516085   0.323915     0.654308     0.857588   \n",
       "748   0.676392   0.560715   0.575147   0.336958     0.699631     0.752875   \n",
       "749   0.331757   0.773867   0.800121   0.176060     0.352877     0.379286   \n",
       "750   0.297934   0.806955   0.564062   0.510765     0.436667     0.348156   \n",
       "751   0.376556   0.671358   0.803143   0.303995     0.243517     0.534543   \n",
       "\n",
       "     Z78285_f_at  \n",
       "0       0.434172  \n",
       "1       0.317450  \n",
       "2       0.460110  \n",
       "3       0.423364  \n",
       "4       0.557378  \n",
       "..           ...  \n",
       "747     0.324064  \n",
       "748     0.557553  \n",
       "749     0.726550  \n",
       "750     0.405826  \n",
       "751     0.648342  \n",
       "\n",
       "[752 rows x 7130 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "# scaler = scaler.fit(X_train)\n",
    "dfNorm = scaler.fit_transform(df)\n",
    "dfNorm = pd.DataFrame(dfNorm, columns = df.columns, index = df.index)\n",
    "X_trainNorm = scaler.fit_transform(X_train)\n",
    "X_trainNorm = pd.DataFrame(X_trainNorm, columns = X_train.columns, index = X_train.index)\n",
    "X_testNorm = scaler.fit_transform(X_test)\n",
    "X_testNorm = pd.DataFrame(X_testNorm, columns = X_test.columns, index = X_test.index)\n",
    "dfNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 The training set contains more predictors than observations. What problem(s) can this lead to in fitting a classification model to such a dataset? Explain in 3 or fewer sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 1.2.:\n",
    "When p > n we obtain perfect collinearity of the predictor set, and our model is susceptible to overfitting. \n",
    "Other problems that can arise with high dimentionality are that matrices may not be invertible (issue in OLS).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Determine which single gene individually discriminates between the two cancer classes the best (consider every gene in the dataset) and call it `best_predictor`.**\n",
    "\n",
    "**Plot two histograms of your `best_predictor` -- one using the training set and another using the testing set. The histogram should clearly distinguish two different `Cancer_type` classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gene that provides the highest R^2 is AFFX-BioC-5_st with a an R^2 score of 0.6702\n"
     ]
    }
   ],
   "source": [
    "# Calculating highest R2 from using single predictor log regression on each gene.\n",
    "\n",
    "list_of_R2= []\n",
    "for row in np.arange(len(y_test)):\n",
    "    logit1 = LogisticRegression(penalty=\"none\", fit_intercept=True).fit(X_trainNorm.iloc[:,row].values.reshape(-1,1), y_train)\n",
    "    list_of_R2.append(accuracy_score(y_test, logit1.predict(X_testNorm.iloc[:,row].values.reshape(-1,1))))\n",
    "\n",
    "best_predictor = X_testNorm.columns[np.argmax(list_of_R2)]\n",
    "print('The gene that provides the highest R^2 is {0} with a an R^2 score of {1:.4f}'.format(best_predictor,np.max(list_of_R2)))                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the values into the ones that are predicted to be ALL and AML respectively:\n",
    "best_pred_ALL=[]\n",
    "best_pred_AML=[]\n",
    "# slett en av disse linjene: logit1 = LogisticRegression(penalty=\"none\", fit_intercept=True).fit(X_trainNorm, y_train)\n",
    "logit1 = LogisticRegression(penalty=\"none\", fit_intercept=True).fit(X_trainNorm.loc[:,best_predictor].values.reshape(-1,1), y_train)\n",
    "for indeks, row in enumerate(X_trainNorm[best_predictor]):\n",
    "    if (1-int(logit1.predict(X_trainNorm.loc[X_trainNorm.index[indeks], best_predictor].reshape(-1,1)))) == 0:\n",
    "        best_pred_ALL.append(row)\n",
    "    else:\n",
    "        best_pred_AML.append(row)\n",
    "# same for test        \n",
    "best_pred_ALL_test=[]\n",
    "best_pred_AML_test=[]\n",
    "for indeks, row in enumerate(X_testNorm[best_predictor]):\n",
    "    if (1-int(logit1.predict(X_testNorm.loc[X_testNorm.index[indeks], best_predictor].reshape(-1,1)))) == 0:\n",
    "        best_pred_ALL_test.append(row)\n",
    "    else:\n",
    "        best_pred_AML_test.append(row)\n",
    "# best_pred_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.hist(X_trainNorm[best_predictor], alpha=0.7, label='Train set')\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ax[0].hist(best_pred_ALL, alpha=0.7, label='Predicted to be ALL', bins=10)\n",
    "ax[0].hist(best_pred_AML, alpha=0.7, label='Predicted to be AML', bins=10)\n",
    "ax[0].vlines(0.5,ymin=0,ymax=85, linestyle='--',color='brown',alpha=0.5, label='value=0.5')\n",
    "ax[0].set_xlabel('values in the gene \\\"best predictor\\\"')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_title('Train set')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].hist(best_pred_ALL_test, alpha=0.7, label='Predicted to be ALL', bins=10)\n",
    "ax[1].hist(best_pred_AML_test, alpha=0.7, label='Predicted to be AML', bins=10)\n",
    "ax[1].set_xlabel('values in the gene \\\"best predictor\\\"')\n",
    "ax[1].vlines(0.5,ymin=0,ymax=25, linestyle='--',color='brown',alpha=0.5, label='value=0.5')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_title('Test set')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#So they are asking us to clearly distinguish between the two classes of cancer, in which we will..\n",
    "# have to make a prediction on a model. In this case I have used the logit1 we fit against..\n",
    "# each of the single predictors separately. Not sure if this correct looking at the gap in the plot.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 Using `best_predictor`, create a classification model by simply eye-balling a value for this gene that would discriminate the two classes the best (do not use an algorithm to determine for you the optimal coefficient or threshold; we are asking you to provide a rough estimate / model by manual inspection). Justify your choice in 1-2 sentences. Report the accuracy of your hand-chosen model on the test set.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The highest value from best_predictor is observation number 116 which is 1 on the standardized scale.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fix\n",
    "X_testNorm.loc[X_testNorm.index[116],best_predictor]\n",
    "np.argmax(X_testNorm[best_predictor])\n",
    "\n",
    "print('''\n",
    "The highest value from best_predictor is observation number 116 which is 1 on the standardized scale.\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 2 [35pts]: Logistic Regression Modeling </b> </div>\n",
    "\n",
    "\n",
    "**2.1** Fit a simple logistic regression model to the training set using the single gene predictor `best_predictor` to predict cancer type.  Carefully interpret the coefficient estimates for this model.\n",
    "\n",
    "*Remember, you need to set the regularization parameter for sklearn's logistic regression function to be a very large value in order to **not** regularize (use `C=100000` or `penalty = \"none\"`).\n",
    "\n",
    "**2.2** Plot the logistic curves for the model in 2.1 ($y$-axis is probability scale, $x$-axis is `best_predictor`).  Interpret this plot: at what values of your `best_predictor` will you predict the patient to have ALL?  How does this compare to your eeballed value from 1.4?\n",
    "\n",
    "**2.3** Calculate the training and test classification accuracies of this model in 2.1. How do these compare to the eye-balled model from 1.4?\n",
    "\n",
    "\n",
    "**2.4** Next, fit a multiple logistic regression model with **all** the gene predictors from the data set (reminder: for this assignment, we are always using the normalized values). How does the classification accuracy of this model compare with the models fitted with a single gene (on both the training and test sets)?  \n",
    "\n",
    "**2.5** Print out and interpret the logistic regression coefficients for  `best_predictor` from both the simple logistic and multiple logistic regression models from the previous two parts.  Do they agree or disagree?  What does this indicate?\n",
    "\n",
    "**2.6** Now let's use regularization to improve the predictions from the multiple logistic regression model. Specifically, use LASSO-like regularization and 5-fold cross-validation to fit the model on the training set (choose between 20 reasonable values of $\\lambda$). Report the classification accuracy on both the training and testing set.\n",
    "\n",
    "**2.7** How many predictors are considered as important features in this regularized model?  What does that say about the full logistic regression model in problem 2.4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Fit a simple logistic regression model to the training set using the single gene predictor `best_predictor` to predict cancer type. Carefully interpret the coefficient estimates for this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intercept for the single reg model is:  1.2684248229969208 . This means that the log odds for type of cancer are ~1.27 when x=0, i.e. when this gene has its lowest value (since the values are normalizaed).\n",
      "The coefficiant for the single reg model is:  -2.5937379099642888 . This means that the log odds for type of cancer decreases with ~2.59 when x goes from 0 to 1, i.e when this gene has the highest observed value rather than the lowest.\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(penalty=\"none\", fit_intercept=True)\n",
    "logit_single = logit.fit(X_trainNorm[best_predictor].values.reshape(-1, 1), y_train)\n",
    "print('The intercept for the single reg model is: ', logit_single.intercept_[0], '. This means that the log odds for type of cancer are ~1.27 when x=0, i.e. when this gene has its lowest value (since the values are normalizaed).')\n",
    "print('The coefficiant for the single reg model is: ', logit_single.coef_[0][0], '. This means that the log odds for type of cancer decreases with ~2.59 when x goes from 0 to 1, i.e when this gene has the highest observed value rather than the lowest.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Plot the logistic curves for the model in 2.1 ($y$-axis is probability scale, $x$-axis is `best_predictor`).  Interpret this plot: at what values of your `best_predictor` will you predict the patient to have ALL?  How does this compare to your eeballed value from 1.4?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slett: X_dummy = np.linspace(np.min(X_testNorm[best_predictor]),np.max(X_testNorm[best_predictor]))\n",
    "classification_boundary = -(logit_single.intercept_[0])/logit_single.coef_[0][0]\n",
    "\n",
    "X_trainNorm_sorted = np.sort(X_trainNorm[best_predictor])\n",
    "X_testNorm_sorted = np.sort(X_testNorm[best_predictor])\n",
    "y_pred_single_train = logit_single.predict(X_trainNorm_sorted.reshape(-1, 1))\n",
    "y_pred_single_test = logit_single.predict(X_testNorm_sorted.reshape(-1, 1))\n",
    "y_predproba_single_train = logit_single.predict_proba(X_trainNorm_sorted.reshape(-1, 1))[:,0]\n",
    "y_predproba_single_test = logit_single.predict_proba(X_testNorm_sorted.reshape(-1, 1))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since the classification boundary is 0.489,the model will predict patients to have ALL for all scores below this value, and AML for all scores above.\n"
     ]
    }
   ],
   "source": [
    "# Plot:\n",
    "\n",
    "plt.plot(X_testNorm_sorted, y_predproba_single_test, label='Train set prediction', alpha=0.5)\n",
    "plt.plot(X_trainNorm_sorted, y_predproba_single_train, label='Test set prediction', alpha=0.5)\n",
    "plt.vlines(classification_boundary,ymin=np.min(X_testNorm_sorted),ymax=np.max(X_testNorm_sorted), linestyles='--', color='g', label='Classification boundary = {:.3f}'.format(classification_boundary))\n",
    "plt.hlines(0.5,xmin=np.min(X_testNorm_sorted),xmax=np.max(X_testNorm_sorted), linestyles='--', color='brown',label='Probability=0.5')\n",
    "plt.xlabel('best_predictor values')\n",
    "plt.ylabel('probability scale')\n",
    "plt.title('Predicted probability of ALL (0) or AML(1) based on best_predictor gene')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Since the classification boundary is {:.3f},the model will predict patients to have ALL for all scores below this value, and AML for all scores above.'.format(classification_boundary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Calculate the training and test classification accuracies of this model in 2.1.  How do these compare to the eye-balled model from 1.4?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accurracy for the train set it: -0.8238188109468385\n",
      "The accurracy for the test set it: -0.8731884057971013\n",
      " They compare to the eye-balled model ......\n"
     ]
    }
   ],
   "source": [
    "print('The accurracy for the train set it:', sk.metrics.r2_score(y_train ,  y_pred_single_train))\n",
    "print('The accurracy for the test set it:', sk.metrics.r2_score(y_test ,  y_pred_single_test))\n",
    "print(' They compare to the eye-balled model ......')  \n",
    "        #fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Next, fit a multiple logistic regression model with *all* the gene predictors from the data set (reminder: for this assignment, we are always using the normalized values). How does the classification accuracy of this model compare with the models fitted with a single gene (on both the training and test sets)?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accurracy for the mutliple regression train set it: 1.0\n",
      "The accurracy for the mutliple regression test set it: 0.1060\n",
      "For comparison the accurracy for the single predictor was 0.6011 for train and 0.6702 for test.\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(penalty=\"none\", fit_intercept=True)\n",
    "logit_multiple = logit.fit(X_trainNorm.values.reshape(-1, 7129), y_train)\n",
    "y_pred_multiple_train = logit_multiple.predict(X_trainNorm)\n",
    "y_pred_multiple_test = logit_multiple.predict(X_testNorm) \n",
    "\n",
    "    \n",
    "print('The accurracy for the mutliple regression train set it:', sk.metrics.r2_score(y_train, y_pred_multiple_train))\n",
    "print('The accurracy for the mutliple regression test set it: {:.4f}'.format(sk.metrics.r2_score(y_test ,  y_pred_multiple_test)))\n",
    "\n",
    "print('For comparison the accurracy for the single predictor was {0:.4f} for train and {1:.4f} for test.'.format(accuracy_score(y_train, logit_single.predict(X_trainNorm.loc[:,best_predictor].values.reshape(-1,1))), np.max(list_of_R2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer 2.4:\n",
    "As we would expect we get an R-squared of 1 for our train-set when we have more predictors than observations. This is clearly overfit, and we get an R-squared of 0.106 the test set. This is much worse than our single predictor model for our best predictor, which got an R^2 score of 0.670 for train and 0.601 for train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Print out and interpret the logistic regression coefficients for `best_predictor` from both the simple logistic and multiple logistic regression models from the previous two parts.  Do they agree or disagree?  What does this indicate?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficient for best_predictor for the multiple regression model is: -1.8751379379201865  \n",
      "This negative coefficient means that a higher value of this predictor indicates a higher probability of having ALL rather than AML (0 rather than 1 in the response)\n",
      "\n",
      "The coefficient for the single reg model is:  -2.5937379099642888 . The reason this coefficien is higher is probably because the single predictor model is explaining variance that has collinearity with other variables that are included in the multiple regression model\n"
     ]
    }
   ],
   "source": [
    "# print(logit_multiple.coef_[0])\n",
    "print('The coefficient for best_predictor for the multiple regression model is:', logit_multiple.coef_[0][X_trainNorm.columns.tolist().index(best_predictor)], ' \\nThis negative coefficient means that a higher value of this predictor indicates a higher probability of having ALL rather than AML (0 rather than 1 in the response)')\n",
    "print('\\nThe coefficient for the single reg model is: ', logit_single.coef_[0][0], '. The reason this coefficien is higher is probably because the single predictor model is explaining variance that has collinearity with other variables that are included in the multiple regression model') \n",
    "\n",
    "#delete this:\n",
    "# print('\\nThe intercept for the multiple regression model is:', logit_multiple.intercept_[0], '.\\nThis defines the log odds for type of cancer when x=0, i.e. when this gene has its lowest value (since the values are normalizaed).')\n",
    "# print('\\nThe intercept for the single reg model is: ', logit_single.intercept_[0],  ' and is individually interpreted in task 2.1.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6 Now let's use regularization to improve the predictions from the multiple logistic regression model. Specifically, use LASSO-like regularization and 5-fold cross-validation to fit the model on the training set (choose between 20 reasonable values of $\\lambda$). Report the classification accuracy on both the training and testing set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training set Score (average of the 5 folds)):  0.5935600237992562\n",
      "Best training set Params:  {'alpha': 0.00379269019073225}\n",
      "Best test set Score (average of the 5 folds):  0.3451886036275259\n",
      "Best test set Params:  {'alpha': 0.00379269019073225}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "lambdas = np.logspace(-3,8,20)\n",
    "\n",
    "\n",
    "def cv_optimize_Lasso(x: np.ndarray, y: np.ndarray, list_of_lambdas: list, n_folds: int = 5):\n",
    "    parameters = {'alpha': list_of_lambdas}\n",
    "    # the scoring parameter below is the default one in ridge, but you can use a different one in the cross-validation phase if you want.\n",
    "    gs = GridSearchCV(Lasso(), param_grid=parameters, cv=n_folds, scoring=\"r2\")\n",
    "    gs.fit(x, y)\n",
    "    return gs\n",
    "\n",
    "lasso_train = cv_optimize_Lasso(X_trainNorm, y_train, lambdas)\n",
    "lasso_test = cv_optimize_Lasso(X_testNorm, y_test, lambdas)\n",
    "\n",
    "print('Best training set Score (average of the 5 folds)): ', lasso_train.best_score_)\n",
    "print('Best training set Params: ', lasso_train.best_params_)\n",
    "print('Best test set Score (average of the 5 folds): ', lasso_test.best_score_)\n",
    "print('Best test set Params: ', lasso_test.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7 How many predictors are considered as important features in this regularized model?  What does that say about the full logistic regression model in problem 2.4?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set Score:  0.7340151757879942\n",
      "Test set Score:  0.4006241929728108\n",
      "Number of coefficiants that have not been shrinked to zero: 107\n"
     ]
    }
   ],
   "source": [
    "lasso_reg = Lasso(alpha= 0.00379269019073225)\n",
    "lasso_reg.fit(X_trainNorm, y_train)\n",
    "\n",
    "print('Training set Score: ', lasso_reg.score(X_trainNorm,y_train))\n",
    "print('Test set Score: ', lasso_reg.score(X_testNorm,y_test))\n",
    "\n",
    "# Making a list of all coefficients that have not been shrinked to 0:\n",
    "coef_list = []\n",
    "for value in lasso_reg.coef_.tolist():\n",
    "    if value != 0:\n",
    "        coef_list.append(value)\n",
    "print('Number of coefficiants that have not been shrinked to zero:',len(coef_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer 2.7:\n",
    "We see that the number of important features has been shrinked from 7129 to 107. We also see that we were correct about the overfitting of the model including all predictors, and that the model is doing much better at prediction with this subset of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 3 [10pts]: $k$-NN Classification </b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Use 5-fold cross-validation to select $k$ for a $k$-NN classification model based on the full predictor set.  Choose between `ks = [1,3,5,7,10,15,20,50,100]`. \n",
    "\n",
    "**3.2** Provide the confusion matrix for 3 models: (i) the full multiple logistic regression model from 2.4, (ii) the best regularized model from 2.6, and (iii) the best $k$-NN from the previous part. Report the false positive and false negative rates (all in the test set).  Briefly interpret what you notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Use 5-fold cross-validation to select $k$ for a $k$-NN classification model based on the full predictor set.  Choose between `ks = [1,3,5,7,10,15,20,50,100]`.  Report your chosen $k$, and report the misclassification rate on both the train and test sets for the model using your chosen $k$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k-value</th>\n",
       "      <th>accurracy score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.771277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.819149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.861702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.861702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.851064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.845745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>0.787234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50</td>\n",
       "      <td>0.712766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>0.601064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k-value  accurracy score\n",
       "0        1         0.771277\n",
       "1        3         0.819149\n",
       "2        5         0.861702\n",
       "3        7         0.861702\n",
       "4       10         0.851064\n",
       "5       15         0.845745\n",
       "6       20         0.787234\n",
       "7       50         0.712766\n",
       "8      100         0.601064"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best k-NN model is when k=5 and k=7 as they are identical in those two situations.\n",
      "Selecting k=5 for my model\n",
      "Score for k=5: 0.8617021276595744\n",
      "Score for k=7: 0.8617021276595744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "n_folds = 5  #antallet folds du vil ha\n",
    "kfold = KFold(n_folds, shuffle=True)\n",
    "ks = [1,3,5,7,10,15,20,50,100]\n",
    "#list(kfold.split(range(48)))\n",
    "\n",
    "score_list = [[],[],[],[],[],[],[],[],[]]\n",
    "for i,k in enumerate(ks):\n",
    "    fittedModel = KNeighborsClassifier(n_neighbors=k).fit(X_trainNorm, y_train)\n",
    "    score_list[i].append(k)\n",
    "    score_list[i].append(fittedModel.score(X_testNorm,y_test))\n",
    "    \n",
    "score_df = pd.DataFrame(score_list, columns=['k-value','accurracy score'])\n",
    "display(score_df)\n",
    "print('The best k-NN model is when k=5 and k=7 as they are identical in those two situations.')\n",
    "print('Selecting k={} for my model'.format(score_df['k-value'][np.argmax(score_df['accurracy score'])]))\n",
    "\n",
    "print('Score for k=5:', score_df['accurracy score'][2])\n",
    "print('Score for k=7:', score_df['accurracy score'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall misclassification rate in train: 0.099290780141844\n",
      "Overall misclassification rate in test: 0.13829787234042556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8617021276595744"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The misclassification rate for k = 5 is (1-accuracy rate):\n",
    "\n",
    "fittedModel_5 = KNeighborsClassifier(n_neighbors=5).fit(X_trainNorm, y_train)\n",
    "print(\"Overall misclassification rate in train:\",1-fittedModel_5.score(X_trainNorm,y_train))\n",
    "print(\"Overall misclassification rate in test:\",1-fittedModel_5.score(X_testNorm,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Provide the confusion matrix for 3 models: (i) the full multiple logistic regression model from 2.4, (ii) the best regularized model from 2.6, and (iii) the best $k$-NN from the previous part. what are the  false positive and false negative rates in these 3 models (all in the test set)?  Briefly interpret what you notice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div class='exercise'><b> Question 4 [15 pts]: Performing Principal Components Analysis </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** Create the full PCA decomposition of `X_train` and apply the transformation to both `X_train` and `X_test`.  Report the shape of both of these.  What is the limiting factor for the maximum number of PCA components for this data set? \n",
    "\n",
    "*Hint: be sure to standardize before performing PCA.\n",
    "\n",
    "**4.2** PCA is often solely used to help in visualizing high-dimensional problems.  Plot the scatterplot of the second PCA vector of train on the $Y$-axis and the first PCA vector of train on the $X$-axis (be sure to denote the classes via different colors and markings).  In 2-3 sentences, explain why using the scatterplot of the top 2 PCA vectors is a useful approach to visualize a high dimensional classification problem.\n",
    "\n",
    "**4.3** Determine and report the variance explained in `X_train` based on the top 2 PCA vectors.  Determine and report how many PCA vectors are needed so that 90\\% of the variability in the predictors is explained, and create a plot to illustrate this result (Hint: look at cumulative explained variability vs. number of PCA components used).  Select a reasonable value for the number of components that balances representativeness (of the predictors) with parsimony. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 Create the full PCA decomposition of X_train and apply the transformation to both X_train and X_test. Report the shape of both of these. What is the limiting factor for the maximum number of PCA components for this data set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 PCA is often solely used to help in visualizing high-dimensional problems. Plot the scatterplot of the second PCA vector on the  ð‘Œ -axis and the first PCA vector on the  ð‘‹ -axis (be sure to denote the classes via different color/markings). In 2-3 sentences, explain why using the scatterplot of the top 2 PCA vectors is a useful approach to visualize a high dimensional classification problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3 Determine and report the variance explained in `X_train` based on the top 2 PCA vectors.  Determine and report how many PCA vectors are needed so that 90\\% of the variability in the predictors is explained, and create a plot to illustrate this result (Hint: look at cumulative explained variability vs. number of PCA components used).  Select a reasonable value for the number of components that balances representativeness (of the predictors) with parsimony.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 5 [10 pts]: Principal Components Regression (PCR) </b></div>\n",
    "\n",
    "**5.1** Fit three separate Logistic Regression models using principal components as the predictors: (1) with just the first 2 PCA vectors, (2) with the number of component vectors you chose from 5.4 above, and (3) with the number of components that explain at least 90% of the variability in the predictor set. How do the classification accuracy values on both the training and test sets compare with these models?\n",
    "\n",
    "**5.2** Use cross-validation to determine the best number of principal components. Try out the 3 values from the previous sub-part and optionally include other values as well. For the best performing model according to cross-validation, interpret what the model says about the relationship between `best_predictor` and `Cancer_type`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1 Fit three separate Logistic Regression models using principal components as the predictors: (1) with just the first 2 PCA vectors, (2) with the number of component vectors you chose from 5.4 above, and (3) with the number of components that explain at least 90% of the variability in the predictor set. How do the classification accuracy values on both the training and test sets compare with these models?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2 Use cross-validation to determine the best number of principal components. Try out the 3 values from the previous sub-part and optionally include other values as well. For the best performing model according to cross-validation, interpret what the model says about the relationship between your `best_predictor` and `Cancer_type`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 6 [15 pts]: Evaluating Classifiers </b></div>\n",
    "\n",
    "**6.1**: Another way to evaluate models in a classification setting is through an Area-under-the-ROC-Curve (AUC). Briefly explain what the AUC and the ROC are trying to do and how this approach differs from evaluating models based on misclassification rate (as you have done thus far in this problem set).\n",
    "\n",
    "**6.2** Evaluate the 'best' models (best based on test misclassification: if there is a tie, choose the 'simplest' model) from each class of classification models using AUC.  That is calculate AUC for the following models:\n",
    "- the best logistic regression model, whether regularized or not (question 2)\n",
    "- the best $k$-NN model (question 3)\n",
    "- the best PCR model (question 5)\n",
    "\n",
    "For the model with the best AUC, plot the ROC. Briefly interpret your plot.\n",
    "\n",
    "**6.3** Based on AUC, is there one clear stand-out winner or are a lot of models similar in prediction?  If you were to predict real cancer patients, how would use these models to predict cancer type?\n",
    "\n",
    "*See extra information about ALL and AML at the bottom of this notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1 Another way to evaluate models in a classification setting is through an Area-under-the-ROC-Curve (AUC). Briefly explain what the AUC and the ROC are trying to do and how this approach differs from evaluating models based on misclassification rate (as you have done thus far in this problem set).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2 use AUC to evaluate the 'best' models (best based on test misclassification: if there is a tie, choose the 'simplest' model) from each class of classification models.  That is calculate AUC for the following models:**\n",
    "- the best logistic regression model, whether regularized or not (question 2)\n",
    "- the best $k$-NN model (question 3)\n",
    "- the best PCR model (question 5)\n",
    "\n",
    "**For the model with the best AUC, plot the ROC. Briefly interpret your plot.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.3 Based on AUC, is there one clear stand-out winner or are a lot of models similar in prediction?  If you were to predict real cancer patients, how would use these models to predict cancer type?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Additional Information**\n",
    "\n",
    "Acute Lymphoblastic Leukemia (ALL):\n",
    "- About 98% of children with ALL go into remission within weeks after starting treatment.\n",
    "- About 90% of those children can be cured. Patients are considered cured after 10 years in remission.\n",
    "\n",
    "Acute Myeloid Leukemia (AML):\n",
    "- In general, children with AML are seen as lower risk than adults. \n",
    "- Around 85 to 90 percent of children with AML will go into remission after induction, according to the American Cancer Society. AML will return in some cases.  \n",
    "- The five-year-survival-rate for children with AML is 60 to 70 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
