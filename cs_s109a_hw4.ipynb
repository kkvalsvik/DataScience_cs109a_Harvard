{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2021-s109a/blob/master/lectures/crest.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Homework 4:  Logistic Regression and PCA\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Summer 2021**<br/>\n",
    "**Instructors**: Kevin Rader\n",
    "\n",
    "\n",
    "<hr style='height:2px'>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #fce8e8;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "- As much as possible, try and stick to the hints and functions we import at the top of the homework, as those are the ideas and tools the class supports and is aiming to teach. And if a problem specifies a particular library you're required to use that library, and possibly others from the import list.\n",
    "- Please use .head() when viewing data. Do not submit a notebook that is excessively long because output was not suppressed or otherwise limited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import zipfile\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# if you want to do a 2-sample t-test:\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='theme'> Cancer Classification from Gene Expressions </div>\n",
    "\n",
    "In this problem, we will build a classification model to distinguish between two related classes of cancer, acute lymphoblastic leukemia (ALL) and acute myeloid leukemia (AML), using gene expression measurements. The dataset is provided in the file `data/genomic_data.csv`. Each row in this file corresponds to a tumor tissue sample from a patient with one of the two forms of Leukemia. The first column contains the cancer type, with **0 indicating the ALL** class and **1 indicating the AML** class. Columns 2-7130 contain expression levels of 7129 genes recorded from each tissue sample. \n",
    "\n",
    "In the following questions, we will use linear and logistic regression to build classification models for this data set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 1 [15 pts]: Data Exploration </b></div>\n",
    "\n",
    "The first step is to split the observations into an approximate 75-25 train-test split.  Below is some code to do this for you (we want to make sure everyone has the same splits). It also prints the dataset's shape before splitting and after splitting. `Cancer_type` is our target column.\n",
    "\n",
    "\n",
    "**1.1** Take a peek at your training set: you should notice the severe differences in the measurements from one gene to the next (some are negative, some hover around zero, and some are well into the thousands). To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1. **NOTE: for the entirety of this homework assignment, you will use these normalized values, not the original, raw values**. Normalizing genomic data is a fairly standard first step.\n",
    "\n",
    "\n",
    "**1.2** The training set contains more predictors than observations. What problem(s) can this lead to in fitting a classification model to such a dataset? Explain in 3 or fewer sentences.\n",
    "\n",
    "\n",
    "**1.3** Determine which single gene individually discriminates between the two cancer classes the best (consider every gene in the dataset) and call it `best_predictor`.\n",
    "\n",
    "Plot two histograms of your `best_predictor` -- one using the training set and another using the testing set. The histogram should clearly distinguish two different `Cancer_type` classes.\n",
    "\n",
    "**Hint:** You may use any reasonable approach to determine the `best_predictor`, but please use something very simple (whether taught in this class or elsewhere).\n",
    "\n",
    "\n",
    "**1.4** Using `best_predictor`, create a classification model by simply eye-balling a value for this gene that would discriminate the two classes the best (do not use an algorithm to determine for you the optimal coefficient or threshold; we are asking you to provide a rough estimate / model by manual inspection). Justify your choice in 1-2 sentences. Report the accuracy of your hand-chosen model on the test set.\n",
    "\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first step is to split the observations into an approximate 75-25 train-test split. Below is some code to do this for you (we want to make sure everyone has the same splits). Print dataset shape before splitting and after splitting. `Cancer_type` is our target column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "df = pd.read_csv('data/genomic_data.csv', index_col=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns != 'Cancer_type'], \n",
    "                                                         df.Cancer_type, test_size=0.25, \n",
    "                                                         random_state = 109, \n",
    "                                                         stratify = df.Cancer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of full set: (752, 7130)\n",
      "shape of x_train: (564, 7129) \n",
      "shape of x_test: (188, 7129) \n",
      "shape of y_train: (564,) \n",
      "shape of y_test: (188,)\n",
      "value count of dataset: \n",
      " 0.0    0.511968\n",
      "1.0    0.488032\n",
      "Name: Cancer_type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('shape of full set:', df.shape)\n",
    "print('shape of x_train:', X_train.shape,'\\nshape of x_test:',X_test.shape,'\\nshape of y_train:', y_train.shape,'\\nshape of y_test:', y_test.shape)\n",
    "print('value count of dataset: \\n', df.Cancer_type.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Take a peek at your training set: you should notice the severe differences in the measurements from one gene to the next (some are negative, some hover around zero, and some are well into the thousands). To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1. **NOTE: for the entirety of this homework assignment, you will use these normalized values, not the original, raw values.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AFFX-BioB-5_at</th>\n",
       "      <th>AFFX-BioB-M_at</th>\n",
       "      <th>AFFX-BioB-3_at</th>\n",
       "      <th>AFFX-BioC-5_at</th>\n",
       "      <th>AFFX-BioC-3_at</th>\n",
       "      <th>AFFX-BioDn-5_at</th>\n",
       "      <th>AFFX-BioDn-3_at</th>\n",
       "      <th>AFFX-CreX-5_at</th>\n",
       "      <th>AFFX-CreX-3_at</th>\n",
       "      <th>AFFX-BioB-5_st</th>\n",
       "      <th>...</th>\n",
       "      <th>U48730_at</th>\n",
       "      <th>U58516_at</th>\n",
       "      <th>U73738_at</th>\n",
       "      <th>X06956_at</th>\n",
       "      <th>X16699_at</th>\n",
       "      <th>X83863_at</th>\n",
       "      <th>Z17240_at</th>\n",
       "      <th>L49218_f_at</th>\n",
       "      <th>M71243_f_at</th>\n",
       "      <th>Z78285_f_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>0.551231</td>\n",
       "      <td>0.491834</td>\n",
       "      <td>0.446073</td>\n",
       "      <td>0.683776</td>\n",
       "      <td>0.582740</td>\n",
       "      <td>0.706226</td>\n",
       "      <td>0.600837</td>\n",
       "      <td>0.279027</td>\n",
       "      <td>0.795057</td>\n",
       "      <td>0.139594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459868</td>\n",
       "      <td>0.475715</td>\n",
       "      <td>0.690460</td>\n",
       "      <td>0.339711</td>\n",
       "      <td>0.138097</td>\n",
       "      <td>0.583479</td>\n",
       "      <td>0.334816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380770</td>\n",
       "      <td>0.752379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.416298</td>\n",
       "      <td>0.582563</td>\n",
       "      <td>0.438845</td>\n",
       "      <td>0.395964</td>\n",
       "      <td>0.580600</td>\n",
       "      <td>0.774472</td>\n",
       "      <td>0.328722</td>\n",
       "      <td>0.419572</td>\n",
       "      <td>0.372083</td>\n",
       "      <td>0.561972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660277</td>\n",
       "      <td>0.655070</td>\n",
       "      <td>0.231898</td>\n",
       "      <td>0.194729</td>\n",
       "      <td>0.708641</td>\n",
       "      <td>0.537605</td>\n",
       "      <td>0.433375</td>\n",
       "      <td>0.550658</td>\n",
       "      <td>0.301676</td>\n",
       "      <td>0.467305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.269916</td>\n",
       "      <td>0.688088</td>\n",
       "      <td>0.376977</td>\n",
       "      <td>0.490495</td>\n",
       "      <td>0.619277</td>\n",
       "      <td>0.625425</td>\n",
       "      <td>0.623749</td>\n",
       "      <td>0.775985</td>\n",
       "      <td>0.515638</td>\n",
       "      <td>0.201543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557309</td>\n",
       "      <td>0.244917</td>\n",
       "      <td>0.428507</td>\n",
       "      <td>0.259558</td>\n",
       "      <td>0.553007</td>\n",
       "      <td>0.338148</td>\n",
       "      <td>0.562251</td>\n",
       "      <td>0.232694</td>\n",
       "      <td>0.470838</td>\n",
       "      <td>0.483762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.578855</td>\n",
       "      <td>0.400594</td>\n",
       "      <td>0.872766</td>\n",
       "      <td>0.205733</td>\n",
       "      <td>0.374758</td>\n",
       "      <td>0.394855</td>\n",
       "      <td>0.670381</td>\n",
       "      <td>0.492747</td>\n",
       "      <td>0.183372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666246</td>\n",
       "      <td>0.597685</td>\n",
       "      <td>0.553157</td>\n",
       "      <td>0.330940</td>\n",
       "      <td>0.692364</td>\n",
       "      <td>0.667400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.376459</td>\n",
       "      <td>0.309709</td>\n",
       "      <td>0.158615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>0.180012</td>\n",
       "      <td>0.536142</td>\n",
       "      <td>0.144838</td>\n",
       "      <td>0.343449</td>\n",
       "      <td>0.411867</td>\n",
       "      <td>0.314464</td>\n",
       "      <td>0.968593</td>\n",
       "      <td>0.420012</td>\n",
       "      <td>0.476040</td>\n",
       "      <td>0.318838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365856</td>\n",
       "      <td>0.656925</td>\n",
       "      <td>0.138411</td>\n",
       "      <td>0.286228</td>\n",
       "      <td>0.556621</td>\n",
       "      <td>0.780834</td>\n",
       "      <td>0.721513</td>\n",
       "      <td>0.522074</td>\n",
       "      <td>0.421493</td>\n",
       "      <td>0.342380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.102031</td>\n",
       "      <td>0.738981</td>\n",
       "      <td>0.597432</td>\n",
       "      <td>0.374311</td>\n",
       "      <td>0.461244</td>\n",
       "      <td>0.430339</td>\n",
       "      <td>0.604786</td>\n",
       "      <td>0.577060</td>\n",
       "      <td>0.627416</td>\n",
       "      <td>0.347594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814914</td>\n",
       "      <td>0.966929</td>\n",
       "      <td>0.818381</td>\n",
       "      <td>0.293913</td>\n",
       "      <td>0.867871</td>\n",
       "      <td>0.818321</td>\n",
       "      <td>0.548880</td>\n",
       "      <td>0.680098</td>\n",
       "      <td>0.521110</td>\n",
       "      <td>0.431207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.574094</td>\n",
       "      <td>0.818686</td>\n",
       "      <td>0.368954</td>\n",
       "      <td>0.393693</td>\n",
       "      <td>0.510911</td>\n",
       "      <td>0.592302</td>\n",
       "      <td>0.731582</td>\n",
       "      <td>0.268844</td>\n",
       "      <td>0.327339</td>\n",
       "      <td>0.465086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666029</td>\n",
       "      <td>0.243248</td>\n",
       "      <td>0.353895</td>\n",
       "      <td>0.110635</td>\n",
       "      <td>0.895056</td>\n",
       "      <td>0.517048</td>\n",
       "      <td>0.465522</td>\n",
       "      <td>0.312341</td>\n",
       "      <td>0.406788</td>\n",
       "      <td>0.324980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0.353467</td>\n",
       "      <td>0.663533</td>\n",
       "      <td>0.668569</td>\n",
       "      <td>0.615557</td>\n",
       "      <td>0.627144</td>\n",
       "      <td>0.144345</td>\n",
       "      <td>0.582416</td>\n",
       "      <td>0.243039</td>\n",
       "      <td>0.529310</td>\n",
       "      <td>0.126011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509497</td>\n",
       "      <td>0.274163</td>\n",
       "      <td>0.188150</td>\n",
       "      <td>0.298216</td>\n",
       "      <td>0.438359</td>\n",
       "      <td>0.605212</td>\n",
       "      <td>0.398731</td>\n",
       "      <td>0.592874</td>\n",
       "      <td>0.460222</td>\n",
       "      <td>0.504823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.564385</td>\n",
       "      <td>0.818292</td>\n",
       "      <td>0.427021</td>\n",
       "      <td>0.712689</td>\n",
       "      <td>0.552737</td>\n",
       "      <td>0.242634</td>\n",
       "      <td>0.456397</td>\n",
       "      <td>0.437528</td>\n",
       "      <td>0.506769</td>\n",
       "      <td>0.645919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403056</td>\n",
       "      <td>0.663272</td>\n",
       "      <td>0.526994</td>\n",
       "      <td>0.211653</td>\n",
       "      <td>0.499265</td>\n",
       "      <td>0.722499</td>\n",
       "      <td>0.568509</td>\n",
       "      <td>0.474708</td>\n",
       "      <td>0.697113</td>\n",
       "      <td>0.489629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>0.871242</td>\n",
       "      <td>0.284231</td>\n",
       "      <td>0.543812</td>\n",
       "      <td>0.597192</td>\n",
       "      <td>0.610617</td>\n",
       "      <td>0.439028</td>\n",
       "      <td>0.303964</td>\n",
       "      <td>0.745012</td>\n",
       "      <td>0.416414</td>\n",
       "      <td>0.384581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673305</td>\n",
       "      <td>0.595149</td>\n",
       "      <td>0.349481</td>\n",
       "      <td>0.354006</td>\n",
       "      <td>0.502190</td>\n",
       "      <td>0.822746</td>\n",
       "      <td>0.579128</td>\n",
       "      <td>0.672988</td>\n",
       "      <td>0.635304</td>\n",
       "      <td>0.288971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 7129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AFFX-BioB-5_at  AFFX-BioB-M_at  AFFX-BioB-3_at  AFFX-BioC-5_at  \\\n",
       "730        0.551231        0.491834        0.446073        0.683776   \n",
       "405        0.416298        0.582563        0.438845        0.395964   \n",
       "146        0.269916        0.688088        0.376977        0.490495   \n",
       "130        0.000000        0.578855        0.400594        0.872766   \n",
       "620        0.180012        0.536142        0.144838        0.343449   \n",
       "..              ...             ...             ...             ...   \n",
       "161        0.102031        0.738981        0.597432        0.374311   \n",
       "393        0.574094        0.818686        0.368954        0.393693   \n",
       "417        0.353467        0.663533        0.668569        0.615557   \n",
       "109        0.564385        0.818292        0.427021        0.712689   \n",
       "525        0.871242        0.284231        0.543812        0.597192   \n",
       "\n",
       "     AFFX-BioC-3_at  AFFX-BioDn-5_at  AFFX-BioDn-3_at  AFFX-CreX-5_at  \\\n",
       "730        0.582740         0.706226         0.600837        0.279027   \n",
       "405        0.580600         0.774472         0.328722        0.419572   \n",
       "146        0.619277         0.625425         0.623749        0.775985   \n",
       "130        0.205733         0.374758         0.394855        0.670381   \n",
       "620        0.411867         0.314464         0.968593        0.420012   \n",
       "..              ...              ...              ...             ...   \n",
       "161        0.461244         0.430339         0.604786        0.577060   \n",
       "393        0.510911         0.592302         0.731582        0.268844   \n",
       "417        0.627144         0.144345         0.582416        0.243039   \n",
       "109        0.552737         0.242634         0.456397        0.437528   \n",
       "525        0.610617         0.439028         0.303964        0.745012   \n",
       "\n",
       "     AFFX-CreX-3_at  AFFX-BioB-5_st  ...  U48730_at  U58516_at  U73738_at  \\\n",
       "730        0.795057        0.139594  ...   0.459868   0.475715   0.690460   \n",
       "405        0.372083        0.561972  ...   0.660277   0.655070   0.231898   \n",
       "146        0.515638        0.201543  ...   0.557309   0.244917   0.428507   \n",
       "130        0.492747        0.183372  ...   0.666246   0.597685   0.553157   \n",
       "620        0.476040        0.318838  ...   0.365856   0.656925   0.138411   \n",
       "..              ...             ...  ...        ...        ...        ...   \n",
       "161        0.627416        0.347594  ...   0.814914   0.966929   0.818381   \n",
       "393        0.327339        0.465086  ...   0.666029   0.243248   0.353895   \n",
       "417        0.529310        0.126011  ...   0.509497   0.274163   0.188150   \n",
       "109        0.506769        0.645919  ...   0.403056   0.663272   0.526994   \n",
       "525        0.416414        0.384581  ...   0.673305   0.595149   0.349481   \n",
       "\n",
       "     X06956_at  X16699_at  X83863_at  Z17240_at  L49218_f_at  M71243_f_at  \\\n",
       "730   0.339711   0.138097   0.583479   0.334816     0.000000     0.380770   \n",
       "405   0.194729   0.708641   0.537605   0.433375     0.550658     0.301676   \n",
       "146   0.259558   0.553007   0.338148   0.562251     0.232694     0.470838   \n",
       "130   0.330940   0.692364   0.667400   1.000000     0.376459     0.309709   \n",
       "620   0.286228   0.556621   0.780834   0.721513     0.522074     0.421493   \n",
       "..         ...        ...        ...        ...          ...          ...   \n",
       "161   0.293913   0.867871   0.818321   0.548880     0.680098     0.521110   \n",
       "393   0.110635   0.895056   0.517048   0.465522     0.312341     0.406788   \n",
       "417   0.298216   0.438359   0.605212   0.398731     0.592874     0.460222   \n",
       "109   0.211653   0.499265   0.722499   0.568509     0.474708     0.697113   \n",
       "525   0.354006   0.502190   0.822746   0.579128     0.672988     0.635304   \n",
       "\n",
       "     Z78285_f_at  \n",
       "730     0.752379  \n",
       "405     0.467305  \n",
       "146     0.483762  \n",
       "130     0.158615  \n",
       "620     0.342380  \n",
       "..           ...  \n",
       "161     0.431207  \n",
       "393     0.324980  \n",
       "417     0.504823  \n",
       "109     0.489629  \n",
       "525     0.288971  \n",
       "\n",
       "[188 rows x 7129 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cancer_type</th>\n",
       "      <th>AFFX-BioB-5_at</th>\n",
       "      <th>AFFX-BioB-M_at</th>\n",
       "      <th>AFFX-BioB-3_at</th>\n",
       "      <th>AFFX-BioC-5_at</th>\n",
       "      <th>AFFX-BioC-3_at</th>\n",
       "      <th>AFFX-BioDn-5_at</th>\n",
       "      <th>AFFX-BioDn-3_at</th>\n",
       "      <th>AFFX-CreX-5_at</th>\n",
       "      <th>AFFX-CreX-3_at</th>\n",
       "      <th>...</th>\n",
       "      <th>U48730_at</th>\n",
       "      <th>U58516_at</th>\n",
       "      <th>U73738_at</th>\n",
       "      <th>X06956_at</th>\n",
       "      <th>X16699_at</th>\n",
       "      <th>X83863_at</th>\n",
       "      <th>Z17240_at</th>\n",
       "      <th>L49218_f_at</th>\n",
       "      <th>M71243_f_at</th>\n",
       "      <th>Z78285_f_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482348</td>\n",
       "      <td>0.636828</td>\n",
       "      <td>0.421504</td>\n",
       "      <td>0.354724</td>\n",
       "      <td>0.541964</td>\n",
       "      <td>0.313961</td>\n",
       "      <td>0.656215</td>\n",
       "      <td>0.557102</td>\n",
       "      <td>0.751598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577873</td>\n",
       "      <td>0.430681</td>\n",
       "      <td>0.346731</td>\n",
       "      <td>0.386057</td>\n",
       "      <td>0.611047</td>\n",
       "      <td>0.499519</td>\n",
       "      <td>0.370208</td>\n",
       "      <td>0.549853</td>\n",
       "      <td>0.470914</td>\n",
       "      <td>0.434172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538320</td>\n",
       "      <td>0.663173</td>\n",
       "      <td>0.653266</td>\n",
       "      <td>0.302249</td>\n",
       "      <td>0.463755</td>\n",
       "      <td>0.298072</td>\n",
       "      <td>0.644340</td>\n",
       "      <td>0.479530</td>\n",
       "      <td>0.602312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624184</td>\n",
       "      <td>0.522397</td>\n",
       "      <td>0.715986</td>\n",
       "      <td>0.349330</td>\n",
       "      <td>0.498395</td>\n",
       "      <td>0.471359</td>\n",
       "      <td>0.310147</td>\n",
       "      <td>0.387588</td>\n",
       "      <td>0.460523</td>\n",
       "      <td>0.317450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.558867</td>\n",
       "      <td>0.655742</td>\n",
       "      <td>0.408589</td>\n",
       "      <td>0.409961</td>\n",
       "      <td>0.582961</td>\n",
       "      <td>0.475211</td>\n",
       "      <td>0.599733</td>\n",
       "      <td>0.611503</td>\n",
       "      <td>0.617755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553454</td>\n",
       "      <td>0.469745</td>\n",
       "      <td>0.542662</td>\n",
       "      <td>0.405702</td>\n",
       "      <td>0.628022</td>\n",
       "      <td>0.407405</td>\n",
       "      <td>0.364542</td>\n",
       "      <td>0.508344</td>\n",
       "      <td>0.449332</td>\n",
       "      <td>0.460110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582957</td>\n",
       "      <td>0.642908</td>\n",
       "      <td>0.633893</td>\n",
       "      <td>0.331939</td>\n",
       "      <td>0.476370</td>\n",
       "      <td>0.318081</td>\n",
       "      <td>0.636519</td>\n",
       "      <td>0.554080</td>\n",
       "      <td>0.658937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447358</td>\n",
       "      <td>0.517868</td>\n",
       "      <td>0.289674</td>\n",
       "      <td>0.345401</td>\n",
       "      <td>0.640367</td>\n",
       "      <td>0.558384</td>\n",
       "      <td>0.428003</td>\n",
       "      <td>0.500796</td>\n",
       "      <td>0.499371</td>\n",
       "      <td>0.423364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341354</td>\n",
       "      <td>0.564548</td>\n",
       "      <td>0.468144</td>\n",
       "      <td>0.292583</td>\n",
       "      <td>0.386808</td>\n",
       "      <td>0.177429</td>\n",
       "      <td>0.518919</td>\n",
       "      <td>0.267971</td>\n",
       "      <td>0.617755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665444</td>\n",
       "      <td>0.464084</td>\n",
       "      <td>0.388716</td>\n",
       "      <td>0.371196</td>\n",
       "      <td>0.703637</td>\n",
       "      <td>0.670006</td>\n",
       "      <td>0.414405</td>\n",
       "      <td>0.593250</td>\n",
       "      <td>0.565236</td>\n",
       "      <td>0.557378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.696528</td>\n",
       "      <td>0.584498</td>\n",
       "      <td>0.379790</td>\n",
       "      <td>0.437056</td>\n",
       "      <td>0.422404</td>\n",
       "      <td>0.294715</td>\n",
       "      <td>0.631741</td>\n",
       "      <td>0.330545</td>\n",
       "      <td>0.747420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560462</td>\n",
       "      <td>0.651464</td>\n",
       "      <td>0.435819</td>\n",
       "      <td>0.311584</td>\n",
       "      <td>0.508865</td>\n",
       "      <td>0.516085</td>\n",
       "      <td>0.323915</td>\n",
       "      <td>0.654308</td>\n",
       "      <td>0.857588</td>\n",
       "      <td>0.324064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.640335</td>\n",
       "      <td>0.709007</td>\n",
       "      <td>0.145601</td>\n",
       "      <td>0.640275</td>\n",
       "      <td>0.543410</td>\n",
       "      <td>0.819101</td>\n",
       "      <td>0.498926</td>\n",
       "      <td>0.753030</td>\n",
       "      <td>0.840198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793460</td>\n",
       "      <td>0.295969</td>\n",
       "      <td>0.402568</td>\n",
       "      <td>0.676392</td>\n",
       "      <td>0.560715</td>\n",
       "      <td>0.575147</td>\n",
       "      <td>0.336958</td>\n",
       "      <td>0.699631</td>\n",
       "      <td>0.752875</td>\n",
       "      <td>0.557553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.519750</td>\n",
       "      <td>0.459566</td>\n",
       "      <td>0.198037</td>\n",
       "      <td>0.314083</td>\n",
       "      <td>0.582719</td>\n",
       "      <td>0.369624</td>\n",
       "      <td>0.658192</td>\n",
       "      <td>0.444315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506640</td>\n",
       "      <td>0.212257</td>\n",
       "      <td>0.585057</td>\n",
       "      <td>0.331757</td>\n",
       "      <td>0.773867</td>\n",
       "      <td>0.800121</td>\n",
       "      <td>0.176060</td>\n",
       "      <td>0.352877</td>\n",
       "      <td>0.379286</td>\n",
       "      <td>0.726550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.622598</td>\n",
       "      <td>0.732820</td>\n",
       "      <td>0.621132</td>\n",
       "      <td>0.303219</td>\n",
       "      <td>0.065618</td>\n",
       "      <td>0.375281</td>\n",
       "      <td>0.557282</td>\n",
       "      <td>0.373386</td>\n",
       "      <td>0.490326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559916</td>\n",
       "      <td>0.536638</td>\n",
       "      <td>0.238878</td>\n",
       "      <td>0.297934</td>\n",
       "      <td>0.806955</td>\n",
       "      <td>0.564062</td>\n",
       "      <td>0.510765</td>\n",
       "      <td>0.436667</td>\n",
       "      <td>0.348156</td>\n",
       "      <td>0.405826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.566346</td>\n",
       "      <td>0.749340</td>\n",
       "      <td>0.520202</td>\n",
       "      <td>0.586949</td>\n",
       "      <td>0.549270</td>\n",
       "      <td>0.179827</td>\n",
       "      <td>0.547697</td>\n",
       "      <td>0.673164</td>\n",
       "      <td>0.734504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614662</td>\n",
       "      <td>0.400230</td>\n",
       "      <td>0.437619</td>\n",
       "      <td>0.376556</td>\n",
       "      <td>0.671358</td>\n",
       "      <td>0.803143</td>\n",
       "      <td>0.303995</td>\n",
       "      <td>0.243517</td>\n",
       "      <td>0.534543</td>\n",
       "      <td>0.648342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752 rows × 7130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cancer_type  AFFX-BioB-5_at  AFFX-BioB-M_at  AFFX-BioB-3_at  \\\n",
       "0            0.0        0.482348        0.636828        0.421504   \n",
       "1            0.0        0.538320        0.663173        0.653266   \n",
       "2            0.0        0.558867        0.655742        0.408589   \n",
       "3            0.0        0.582957        0.642908        0.633893   \n",
       "4            0.0        0.341354        0.564548        0.468144   \n",
       "..           ...             ...             ...             ...   \n",
       "747          0.0        0.696528        0.584498        0.379790   \n",
       "748          1.0        0.640335        0.709007        0.145601   \n",
       "749          1.0        0.519750        0.459566        0.198037   \n",
       "750          0.0        0.622598        0.732820        0.621132   \n",
       "751          1.0        0.566346        0.749340        0.520202   \n",
       "\n",
       "     AFFX-BioC-5_at  AFFX-BioC-3_at  AFFX-BioDn-5_at  AFFX-BioDn-3_at  \\\n",
       "0          0.354724        0.541964         0.313961         0.656215   \n",
       "1          0.302249        0.463755         0.298072         0.644340   \n",
       "2          0.409961        0.582961         0.475211         0.599733   \n",
       "3          0.331939        0.476370         0.318081         0.636519   \n",
       "4          0.292583        0.386808         0.177429         0.518919   \n",
       "..              ...             ...              ...              ...   \n",
       "747        0.437056        0.422404         0.294715         0.631741   \n",
       "748        0.640275        0.543410         0.819101         0.498926   \n",
       "749        0.314083        0.582719         0.369624         0.658192   \n",
       "750        0.303219        0.065618         0.375281         0.557282   \n",
       "751        0.586949        0.549270         0.179827         0.547697   \n",
       "\n",
       "     AFFX-CreX-5_at  AFFX-CreX-3_at  ...  U48730_at  U58516_at  U73738_at  \\\n",
       "0          0.557102        0.751598  ...   0.577873   0.430681   0.346731   \n",
       "1          0.479530        0.602312  ...   0.624184   0.522397   0.715986   \n",
       "2          0.611503        0.617755  ...   0.553454   0.469745   0.542662   \n",
       "3          0.554080        0.658937  ...   0.447358   0.517868   0.289674   \n",
       "4          0.267971        0.617755  ...   0.665444   0.464084   0.388716   \n",
       "..              ...             ...  ...        ...        ...        ...   \n",
       "747        0.330545        0.747420  ...   0.560462   0.651464   0.435819   \n",
       "748        0.753030        0.840198  ...   0.793460   0.295969   0.402568   \n",
       "749        0.444315        1.000000  ...   0.506640   0.212257   0.585057   \n",
       "750        0.373386        0.490326  ...   0.559916   0.536638   0.238878   \n",
       "751        0.673164        0.734504  ...   0.614662   0.400230   0.437619   \n",
       "\n",
       "     X06956_at  X16699_at  X83863_at  Z17240_at  L49218_f_at  M71243_f_at  \\\n",
       "0     0.386057   0.611047   0.499519   0.370208     0.549853     0.470914   \n",
       "1     0.349330   0.498395   0.471359   0.310147     0.387588     0.460523   \n",
       "2     0.405702   0.628022   0.407405   0.364542     0.508344     0.449332   \n",
       "3     0.345401   0.640367   0.558384   0.428003     0.500796     0.499371   \n",
       "4     0.371196   0.703637   0.670006   0.414405     0.593250     0.565236   \n",
       "..         ...        ...        ...        ...          ...          ...   \n",
       "747   0.311584   0.508865   0.516085   0.323915     0.654308     0.857588   \n",
       "748   0.676392   0.560715   0.575147   0.336958     0.699631     0.752875   \n",
       "749   0.331757   0.773867   0.800121   0.176060     0.352877     0.379286   \n",
       "750   0.297934   0.806955   0.564062   0.510765     0.436667     0.348156   \n",
       "751   0.376556   0.671358   0.803143   0.303995     0.243517     0.534543   \n",
       "\n",
       "     Z78285_f_at  \n",
       "0       0.434172  \n",
       "1       0.317450  \n",
       "2       0.460110  \n",
       "3       0.423364  \n",
       "4       0.557378  \n",
       "..           ...  \n",
       "747     0.324064  \n",
       "748     0.557553  \n",
       "749     0.726550  \n",
       "750     0.405826  \n",
       "751     0.648342  \n",
       "\n",
       "[752 rows x 7130 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "# scaler = scaler.fit(X_train)\n",
    "dfNorm = scaler.fit_transform(df)\n",
    "dfNorm = pd.DataFrame(dfNorm, columns = df.columns, index = df.index)\n",
    "X_trainNorm = scaler.fit_transform(X_train)\n",
    "X_trainNorm = pd.DataFrame(X_trainNorm, columns = X_train.columns, index = X_train.index)\n",
    "X_testNorm = scaler.fit_transform(X_test)\n",
    "X_testNorm = pd.DataFrame(X_testNorm, columns = X_test.columns, index = X_test.index)\n",
    "dfNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 The training set contains more predictors than observations. What problem(s) can this lead to in fitting a classification model to such a dataset? Explain in 3 or fewer sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 1.2.:\n",
    "When p > n we obtain perfect collinearity of the predictor set, and our model is susceptible to overfitting. \n",
    "Other problems that can arise with high dimentionality are that matrices may not be invertible (issue in OLS).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Determine which single gene individually discriminates between the two cancer classes the best (consider every gene in the dataset) and call it `best_predictor`.**\n",
    "\n",
    "**Plot two histograms of your `best_predictor` -- one using the training set and another using the testing set. The histogram should clearly distinguish two different `Cancer_type` classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9583333333333333"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scribbles - delete\n",
    "X_trainNorm.shape\n",
    "logit1 = LogisticRegression(penalty=\"none\", fit_intercept=True).fit(X_trainNorm.iloc[:,2].values.reshape(-1,1), y_train)\n",
    "sk.metrics.r2_score(y_test, logit1.predict(X_testNorm.iloc[:,2].values.reshape(-1,1)))\n",
    "# logit1.predict(X_testNorm.iloc[:,2].values.reshape(-1,1))\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score gave this as the best:      AFFX-BioC-5_st\n",
      "sk.metrics.r2_score gave this as the best: AFFX-BioC-5_st. Still it is negative, and better to use y-bar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AFFX-BioC-5_st'"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating highest R2 from using single predictor log regression on each gene.\n",
    "\n",
    "logit1 = LogisticRegression(penalty=\"none\", fit_intercept=True).fit(X_trainNorm, y_train)\n",
    "highestBeta = np.argmin(logit1.coef_) #dette er høyeste beta\n",
    "# len(logit1.coef_[0])\n",
    "# highestBeta\n",
    "X_trainNorm.columns[highestBeta]\n",
    "list_of_R2= []\n",
    "for row in np.arange(len(y_test)):\n",
    "    logit1 = LogisticRegression(penalty=\"none\", fit_intercept=True).fit(X_trainNorm.iloc[:,row].values.reshape(-1,1), y_train)\n",
    "    list_of_R2.append(accuracy_score(y_test, logit1.predict(X_testNorm.iloc[:,row].values.reshape(-1,1))))\n",
    "# tror det er feil, da dette viser HØYEST(negativ) beta, men sjekke mot svaret jeg får for \"best\" beta ved regressjon.\n",
    "# list_of_R2\n",
    "np.max(list_of_R2)\n",
    "np.argmax(list_of_R2)\n",
    "print('Accuracy_score gave this as the best:      AFFX-BioC-5_st')\n",
    "print('sk.metrics.r2_score gave this as the best: AFFX-BioC-5_st. Still it is negative, and better to use y-bar')\n",
    "X_testNorm.columns[np.argmax(list_of_R2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the values into the ones that are predicted to be ALL and AML respectively:\n",
    "best_pred_ALL=[]\n",
    "best_pred_AML=[]\n",
    "for indeks, row in enumerate(X_trainNorm[best_predictor]):\n",
    "    if (1-int(logit1.predict(X_trainNorm.loc[X_trainNorm.index[indeks], best_predictor].reshape(-1,1)))) == 0:\n",
    "        best_pred_ALL.append(row)\n",
    "    else:\n",
    "        best_pred_AML.append(row)\n",
    "# same for test        \n",
    "best_pred_ALL_test=[]\n",
    "best_pred_AML_test=[]\n",
    "for indeks, row in enumerate(X_testNorm[best_predictor]):\n",
    "    if (1-int(logit1.predict(X_testNorm.loc[X_testNorm.index[indeks], best_predictor].reshape(-1,1)))) == 0:\n",
    "        best_pred_ALL_test.append(row)\n",
    "    else:\n",
    "        best_pred_AML_test.append(row)\n",
    "# best_pred_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slett:\n",
    "\n",
    "# best_predictor = X_testNorm.columns[np.argmax(list_of_R2)]\n",
    "# best_predictor\n",
    "\n",
    "# # X_trainNorm[best_predictor].index(logit1.predict(X_trainNorm[best_predictor].values.reshape(-1,1))==1)\n",
    "# np.unique(pd.DataFrame(logit1.predict(X_trainNorm[best_predictor].values.reshape(-1,1))),return_index=True,return_counts=True)[1]\n",
    "\n",
    "# # creating separate set for values predicted to ALL and AML:\n",
    "# best_pred_AML = logit1.predict(X_trainNorm[best_predictor].values.reshape(-1,1))*X_trainNorm[best_predictor]\n",
    "# best_pred_ALL =  lambda X_trainNorm[best_predictor]: (1-logit1.predict(X_trainNorm[best_predictor].values.reshape(-1,1))) if X_trainNorm[best_predictor]==0 \n",
    "# best_pred_ALL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.hist(X_trainNorm[best_predictor], alpha=0.7, label='Train set')\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ax[0].hist(best_pred_ALL, alpha=0.7, label='Predicted to be ALL', bins=10)\n",
    "ax[0].hist(best_pred_AML, alpha=0.7, label='Predicted to be AML', bins=10)\n",
    "ax[0].set_xlabel('values in the gene \\\"best predictor\\\"')\n",
    "ax[0].set_ylabel('Count')\n",
    "ax[0].set_title('Train set')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].hist(best_pred_ALL_test, alpha=0.7, label='Predicted to be ALL', bins=10)\n",
    "ax[1].hist(best_pred_AML_test, alpha=0.7, label='Predicted to be AML', bins=10)\n",
    "ax[1].set_xlabel('values in the gene \\\"best predictor\\\"')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_title('Test set')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all this\n",
    "\n",
    "#Plot two histograms of your best_predictor --\n",
    "#one using the training set and another using the testing set.\n",
    "#The histogram should clearly distinguish two different Cancer_type classes.\n",
    "\n",
    "#we might want to plot the values instead, and first categorize them in which one are predicted to 1\n",
    "\n",
    "# plt.hist(X_trainNorm[best_predictor], alpha=0.7, label='Train set')\n",
    "# plt.hist(logit1.predict(X_trainNorm[best_predictor].values.reshape(-1,1)), alpha=0.7, label='Train set')\n",
    "# plt.hist(logit1.predict(X_testNorm[best_predictor].values.reshape(-1,1)), alpha=0.7, label='Test set')\n",
    "# plt.xlabel('prediction of ALL (0) or AML (1) from the gene \\\"best predictor\\\"')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Comparison of distribution of predictions of gene in train and test set')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.718446785368679\n",
      "0.2684675722196719\n",
      "0.7114533099610166\n",
      "0.26198710609369513\n",
      "The column with highest mean from set 1 is  5455\n",
      "The column with highest mean from set 0 is  1475\n",
      "The column with lowest mean from set 1 is  661\n",
      "The column with lowest mean from set 0 is  4078\n"
     ]
    }
   ],
   "source": [
    "# dette er feil, slett\n",
    "dfType1 = dfNorm[dfNorm['Cancer_type']==1].drop(columns='Cancer_type')\n",
    "dfType0 = dfNorm[dfNorm['Cancer_type']==0].drop(columns='Cancer_type')\n",
    "\n",
    "print(np.max(dfType0.mean()))\n",
    "print(np.min(dfType0.mean()))\n",
    "\n",
    "print(np.max(dfType1.mean()))\n",
    "print(np.min(dfType1.mean()))\n",
    "\n",
    "print('The column with highest mean from set 1 is ',np.argmax(dfType1.mean()))\n",
    "print('The column with highest mean from set 0 is ',np.argmax(dfType0.mean()))\n",
    "print('The column with lowest mean from set 1 is ',np.argmin(dfType1.mean()))\n",
    "print('The column with lowest mean from set 0 is ',np.argmin(dfType0.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 Using `best_predictor`, create a classification model by simply eye-balling a value for this gene that would discriminate the two classes the best (do not use an algorithm to determine for you the optimal coefficient or threshold; we are asking you to provide a rough estimate / model by manual inspection). Justify your choice in 1-2 sentences. Report the accuracy of your hand-chosen model on the test set.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The highest value from best_predictor is observation number 116 which is 1 on the standardized scale.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fix\n",
    "X_testNorm.loc[X_testNorm.index[116],best_predictor]\n",
    "np.argmax(X_testNorm[best_predictor])\n",
    "\n",
    "print('''\n",
    "The highest value from best_predictor is observation number 116 which is 1 on the standardized scale.\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 2 [35pts]: Logistic Regression Modeling </b> </div>\n",
    "\n",
    "\n",
    "**2.1** Fit a simple logistic regression model to the training set using the single gene predictor `best_predictor` to predict cancer type.  Carefully interpret the coefficient estimates for this model.\n",
    "\n",
    "*Remember, you need to set the regularization parameter for sklearn's logistic regression function to be a very large value in order to **not** regularize (use `C=100000` or `penalty = \"none\"`).\n",
    "\n",
    "**2.2** Plot the logistic curves for the model in 2.1 ($y$-axis is probability scale, $x$-axis is `best_predictor`).  Interpret this plot: at what values of your `best_predictor` will you predict the patient to have ALL?  How does this compare to your eeballed value from 1.4?\n",
    "\n",
    "**2.3** Calculate the training and test classification accuracies of this model in 2.1. How do these compare to the eye-balled model from 1.4?\n",
    "\n",
    "\n",
    "**2.4** Next, fit a multiple logistic regression model with **all** the gene predictors from the data set (reminder: for this assignment, we are always using the normalized values). How does the classification accuracy of this model compare with the models fitted with a single gene (on both the training and test sets)?  \n",
    "\n",
    "**2.5** Print out and interpret the logistic regression coefficients for  `best_predictor` from both the simple logistic and multiple logistic regression models from the previous two parts.  Do they agree or disagree?  What does this indicate?\n",
    "\n",
    "**2.6** Now let's use regularization to improve the predictions from the multiple logistic regression model. Specifically, use LASSO-like regularization and 5-fold cross-validation to fit the model on the training set (choose between 20 reasonable values of $\\lambda$). Report the classification accuracy on both the training and testing set.\n",
    "\n",
    "**2.7** How many predictors are considered as important features in this regularized model?  What does that say about the full logistic regression model in problem 2.4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Fit a simple logistic regression model to the training set using the single gene predictor `best_predictor` to predict cancer type. Carefully interpret the coefficient estimates for this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intercept for the single reg model is:  1.2684248229969208 . This means that the log odds for type of cancer are ~1.27 when x=0, i.e. when this gene has its lowest value (since the values are normalizaed).\n",
      "The coefficiant for the single reg model is:  -2.5937379099642888 . This means that the log odds for type of cancer decreases with ~2.59 when x goes from 0 to 1, i.e when this gene has the highest observed value rather than the lowest.\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(penalty=\"none\", fit_intercept=True)\n",
    "logit_single = logit.fit(X_trainNorm[best_predictor].values.reshape(-1, 1), y_train)\n",
    "\n",
    "print('The intercept for the single reg model is: ', logit_single.intercept_[0], '. This means that the log odds for type of cancer are ~1.27 when x=0, i.e. when this gene has its lowest value (since the values are normalizaed).')\n",
    "print('The coefficiant for the single reg model is: ', logit_single.coef_[0][0], '. This means that the log odds for type of cancer decreases with ~2.59 when x goes from 0 to 1, i.e when this gene has the highest observed value rather than the lowest.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Plot the logistic curves for the model in 2.1 ($y$-axis is probability scale, $x$-axis is `best_predictor`).  Interpret this plot: at what values of your `best_predictor` will you predict the patient to have ALL?  How does this compare to your eeballed value from 1.4?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slett: X_dummy = np.linspace(np.min(X_testNorm[best_predictor]),np.max(X_testNorm[best_predictor]))\n",
    "classification_boundary = -(logit_single.intercept_[0])/logit_single.coef_[0][0]\n",
    "\n",
    "X_trainNorm_sorted = np.sort(X_trainNorm[best_predictor])\n",
    "X_testNorm_sorted = np.sort(X_testNorm[best_predictor])\n",
    "y_pred_single_train = logit_single.predict(X_trainNorm_sorted.reshape(-1, 1))\n",
    "y_pred_single_test = logit_single.predict(X_testNorm_sorted.reshape(-1, 1))\n",
    "y_predproba_single_train = logit_single.predict_proba(X_trainNorm_sorted.reshape(-1, 1))[:,0]\n",
    "y_predproba_single_test = logit_single.predict_proba(X_testNorm_sorted.reshape(-1, 1))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since the classification boundary is 0.489, the model will predict patients to have ALL for all scored below this value, and AML for all scores above.\n"
     ]
    }
   ],
   "source": [
    "# Plot:\n",
    "\n",
    "plt.plot(X_testNorm_sorted, y_predproba_single_test, label='Train set prediction', alpha=0.5)\n",
    "plt.plot(X_trainNorm_sorted, y_predproba_single_train, label='Test set prediction', alpha=0.5)\n",
    "plt.vlines(classification_boundary,ymin=np.min(X_testNorm_sorted),ymax=np.max(X_testNorm_sorted), linestyles='--', color='g', label='Classification boundary = {:.3f}'.format(classification_boundary))\n",
    "plt.hlines(0.5,xmin=np.min(X_testNorm_sorted),xmax=np.max(X_testNorm_sorted), linestyles='--', color='brown',label='Probability=0.5')\n",
    "plt.xlabel('best_predictor values')\n",
    "plt.ylabel('probability scale')\n",
    "plt.title('Predicted probability of ALL (0) or AML(1) based on best_predictor gene')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Since the classification boundary is {:.3f}, the model will predict patients to have ALL for all scored below this value, and AML for all scores above.'.format(classification_boundary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Calculate the training and test classification accuracies of this model in 2.1.  How do these compare to the eye-balled model from 1.4?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accurracy for the train set it: -0.8238188109468385\n",
      "The accurracy for the test set it: -0.8731884057971013\n",
      " They compare to the eye-balled model ......\n"
     ]
    }
   ],
   "source": [
    "print('The accurracy for the train set it:', sk.metrics.r2_score(y_train ,  y_pred_single_train))\n",
    "print('The accurracy for the test set it:', sk.metrics.r2_score(y_test ,  y_pred_single_test))\n",
    "print(' They compare to the eye-balled model ......')  \n",
    "        #fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Next, fit a multiple logistic regression model with *all* the gene predictors from the data set (reminder: for this assignment, we are always using the normalized values). How does the classification accuracy of this model compare with the models fitted with a single gene (on both the training and test sets)?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accurracy for the train set it: 1.0\n",
      "The accurracy for the test set it: 0.10597826086956519\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression(penalty=\"none\", fit_intercept=True)\n",
    "logit_multiple = logit.fit(X_trainNorm.values.reshape(-1, 7129), y_train)\n",
    "y_pred_multiple_train = logit_multiple.predict(X_trainNorm)\n",
    "y_pred_multiple_test = logit_multiple.predict(X_testNorm) \n",
    "\n",
    "    \n",
    "print('The accurracy for the train set it:', sk.metrics.r2_score(y_train, y_pred_multiple_train))\n",
    "print('The accurracy for the test set it:', sk.metrics.r2_score(y_test ,  y_pred_multiple_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer 2.4:\n",
    "As we would expect we get an R-squared for our train-set when we have more predictors than observations. This is clearly overfit, and we get a much lower R-squared on the test set. However this is a better model than our simple regression model on \"best predictor\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Print out and interpret the logistic regression coefficients for `best_predictor` from both the simple logistic and multiple logistic regression models from the previous two parts.  Do they agree or disagree?  What does this indicate?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients for the multiple regression model is:\n",
      " [ 3.65104993  1.69644023 -1.32174481 ... -0.53882007  4.14380294\n",
      "  1.08813499] .\n",
      "Each of these betas define how much the probability of type of cancer changes when the associated x is increased from 0 to 1, holding all other factors constant.\n",
      "\n",
      "The intercept for the multiple regression model is: -0.12937187791625676 .\n",
      "This defines the log odds for type of cancer when x=0, i.e. when this gene has its lowest value (since the values are normalizaed).\n",
      "\n",
      "The coefficient for the single reg model is:  -2.5937379099642888  and is individually interpreted in task 2.1.\n",
      "\n",
      "The intercept for the single reg model is:  1.2684248229969208  and is individually interpreted in task 2.1.\n"
     ]
    }
   ],
   "source": [
    "# print(logit_multiple.coef_[0])\n",
    "print('The coefficients for the multiple regression model is:\\n', logit_multiple.coef_[0], '.\\nEach of these betas define how much the probability of type of cancer changes when the associated x is increased from 0 to 1, holding all other factors constant.')\n",
    "\n",
    "print('\\nThe intercept for the multiple regression model is:', logit_multiple.intercept_[0], '.\\nThis defines the log odds for type of cancer when x=0, i.e. when this gene has its lowest value (since the values are normalizaed).')\n",
    "\n",
    "print('\\nThe coefficient for the single reg model is: ', logit_single.coef_[0][0], ' and is individually interpreted in task 2.1.') \n",
    "print('\\nThe intercept for the single reg model is: ', logit_single.intercept_[0],  ' and is individually interpreted in task 2.1.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6 Now let's use regularization to improve the predictions from the multiple logistic regression model. Specifically, use LASSO-like regularization and 5-fold cross-validation to fit the model on the training set (choose between 20 reasonable values of $\\lambda$). Report the classification accuracy on both the training and testing set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "lambdas = np.logspace(-3,8,20)\n",
    "\n",
    "\n",
    "def cv_optimize_Lasso(x: np.ndarray, y: np.ndarray, list_of_lambdas: list, n_folds: int = 5):\n",
    "    parameters = {'alpha': list_of_lambdas}\n",
    "    # the scoring parameter below is the default one in ridge, but you can use a different one in the cross-validation phase if you want.\n",
    "    gs = GridSearchCV(Lasso(), param_grid=parameters, cv=n_folds, scoring=\"r2\")\n",
    "    gs.fit(x, y)\n",
    "    return gs\n",
    "\n",
    "lasso_train = cv_optimize_Lasso(X_trainNorm, y_train, lambdas)\n",
    "lasso_test = cv_optimize_Lasso(X_testNorm, y_test, lambdas)\n",
    "\n",
    "print('Best training set Score: ', lasso_train.best_score_)\n",
    "print('Best training set Params: ', lasso_train.best_params_)\n",
    "print('Best test set Score: ', lasso_test.best_score_)\n",
    "print('Best test set Params: ', lasso_test.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7 How many predictors are considered as important features in this regularized model?  What does that say about the full logistic regression model in problem 2.4?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 3 [10pts]: $k$-NN Classification </b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Use 5-fold cross-validation to select $k$ for a $k$-NN classification model based on the full predictor set.  Choose between `ks = [1,3,5,7,10,15,20,50,100]`. \n",
    "\n",
    "**3.2** Provide the confusion matrix for 3 models: (i) the full multiple logistic regression model from 2.4, (ii) the best regularized model from 2.6, and (iii) the best $k$-NN from the previous part. Report the false positive and false negative rates (all in the test set).  Briefly interpret what you notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Use 5-fold cross-validation to select $k$ for a $k$-NN classification model based on the full predictor set.  Choose between `ks = [1,3,5,7,10,15,20,50,100]`.  Report your chosen $k$, and report the misclassification rate on both the train and test sets for the model using your chosen $k$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Provide the confusion matrix for 3 models: (i) the full multiple logistic regression model from 2.4, (ii) the best regularized model from 2.6, and (iii) the best $k$-NN from the previous part. what are the  false positive and false negative rates in these 3 models (all in the test set)?  Briefly interpret what you notice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <div class='exercise'><b> Question 4 [15 pts]: Performing Principal Components Analysis </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** Create the full PCA decomposition of `X_train` and apply the transformation to both `X_train` and `X_test`.  Report the shape of both of these.  What is the limiting factor for the maximum number of PCA components for this data set? \n",
    "\n",
    "*Hint: be sure to standardize before performing PCA.\n",
    "\n",
    "**4.2** PCA is often solely used to help in visualizing high-dimensional problems.  Plot the scatterplot of the second PCA vector of train on the $Y$-axis and the first PCA vector of train on the $X$-axis (be sure to denote the classes via different colors and markings).  In 2-3 sentences, explain why using the scatterplot of the top 2 PCA vectors is a useful approach to visualize a high dimensional classification problem.\n",
    "\n",
    "**4.3** Determine and report the variance explained in `X_train` based on the top 2 PCA vectors.  Determine and report how many PCA vectors are needed so that 90\\% of the variability in the predictors is explained, and create a plot to illustrate this result (Hint: look at cumulative explained variability vs. number of PCA components used).  Select a reasonable value for the number of components that balances representativeness (of the predictors) with parsimony. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 Create the full PCA decomposition of X_train and apply the transformation to both X_train and X_test. Report the shape of both of these. What is the limiting factor for the maximum number of PCA components for this data set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 PCA is often solely used to help in visualizing high-dimensional problems. Plot the scatterplot of the second PCA vector on the  𝑌 -axis and the first PCA vector on the  𝑋 -axis (be sure to denote the classes via different color/markings). In 2-3 sentences, explain why using the scatterplot of the top 2 PCA vectors is a useful approach to visualize a high dimensional classification problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3 Determine and report the variance explained in `X_train` based on the top 2 PCA vectors.  Determine and report how many PCA vectors are needed so that 90\\% of the variability in the predictors is explained, and create a plot to illustrate this result (Hint: look at cumulative explained variability vs. number of PCA components used).  Select a reasonable value for the number of components that balances representativeness (of the predictors) with parsimony.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 5 [10 pts]: Principal Components Regression (PCR) </b></div>\n",
    "\n",
    "**5.1** Fit three separate Logistic Regression models using principal components as the predictors: (1) with just the first 2 PCA vectors, (2) with the number of component vectors you chose from 5.4 above, and (3) with the number of components that explain at least 90% of the variability in the predictor set. How do the classification accuracy values on both the training and test sets compare with these models?\n",
    "\n",
    "**5.2** Use cross-validation to determine the best number of principal components. Try out the 3 values from the previous sub-part and optionally include other values as well. For the best performing model according to cross-validation, interpret what the model says about the relationship between `best_predictor` and `Cancer_type`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1 Fit three separate Logistic Regression models using principal components as the predictors: (1) with just the first 2 PCA vectors, (2) with the number of component vectors you chose from 5.4 above, and (3) with the number of components that explain at least 90% of the variability in the predictor set. How do the classification accuracy values on both the training and test sets compare with these models?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2 Use cross-validation to determine the best number of principal components. Try out the 3 values from the previous sub-part and optionally include other values as well. For the best performing model according to cross-validation, interpret what the model says about the relationship between your `best_predictor` and `Cancer_type`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 6 [15 pts]: Evaluating Classifiers </b></div>\n",
    "\n",
    "**6.1**: Another way to evaluate models in a classification setting is through an Area-under-the-ROC-Curve (AUC). Briefly explain what the AUC and the ROC are trying to do and how this approach differs from evaluating models based on misclassification rate (as you have done thus far in this problem set).\n",
    "\n",
    "**6.2** Evaluate the 'best' models (best based on test misclassification: if there is a tie, choose the 'simplest' model) from each class of classification models using AUC.  That is calculate AUC for the following models:\n",
    "- the best logistic regression model, whether regularized or not (question 2)\n",
    "- the best $k$-NN model (question 3)\n",
    "- the best PCR model (question 5)\n",
    "\n",
    "For the model with the best AUC, plot the ROC. Briefly interpret your plot.\n",
    "\n",
    "**6.3** Based on AUC, is there one clear stand-out winner or are a lot of models similar in prediction?  If you were to predict real cancer patients, how would use these models to predict cancer type?\n",
    "\n",
    "*See extra information about ALL and AML at the bottom of this notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1 Another way to evaluate models in a classification setting is through an Area-under-the-ROC-Curve (AUC). Briefly explain what the AUC and the ROC are trying to do and how this approach differs from evaluating models based on misclassification rate (as you have done thus far in this problem set).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2 use AUC to evaluate the 'best' models (best based on test misclassification: if there is a tie, choose the 'simplest' model) from each class of classification models.  That is calculate AUC for the following models:**\n",
    "- the best logistic regression model, whether regularized or not (question 2)\n",
    "- the best $k$-NN model (question 3)\n",
    "- the best PCR model (question 5)\n",
    "\n",
    "**For the model with the best AUC, plot the ROC. Briefly interpret your plot.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.3 Based on AUC, is there one clear stand-out winner or are a lot of models similar in prediction?  If you were to predict real cancer patients, how would use these models to predict cancer type?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Additional Information**\n",
    "\n",
    "Acute Lymphoblastic Leukemia (ALL):\n",
    "- About 98% of children with ALL go into remission within weeks after starting treatment.\n",
    "- About 90% of those children can be cured. Patients are considered cured after 10 years in remission.\n",
    "\n",
    "Acute Myeloid Leukemia (AML):\n",
    "- In general, children with AML are seen as lower risk than adults. \n",
    "- Around 85 to 90 percent of children with AML will go into remission after induction, according to the American Cancer Society. AML will return in some cases.  \n",
    "- The five-year-survival-rate for children with AML is 60 to 70 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
